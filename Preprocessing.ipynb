{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"7THz_A66T3mM"},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"vT6lshggT1YU"},"outputs":[],"source":["import json, itertools, re, os, glob\n","import pandas as pd\n","import numpy as np\n","import networkx as nx\n","from collections import Counter, defaultdict\n","from statistics import mode"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"THJe_vV4T6OK"},"source":["## Functions"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"BP8O9sm2slZf"},"source":["### Utilities/Helpers"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"SCEUTXUMWjeB"},"outputs":[],"source":["def check_inputs(dat, typ, cd_top):\n","  DATASETS = ['euro', 'timme', 'cd', 'conref']\n","  TYPES = ['full', 'pc']\n","  TOPICS = ['all', 'abortion', 'marijuana', 'gayRights', 'obama']\n","\n","  if dat not in DATASETS:\n","    dat = 'euro'\n","  if typ not in TYPES:\n","    typ = 'full'\n","  if cd_top not in TOPICS:\n","    cd_top = 'all'\n","  \n","  return dat, typ, cd_top"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Abeu7RqWVu4b"},"outputs":[],"source":["def set_mapping(dat):\n","  if dat == 'euro':\n","    mapping = None\n","  elif dat == 'timme':\n","    mapping = None\n","  elif dat == 'cd':\n","    mapping = {-1 : 1, 1 : 2, 0 : 0}\n","  elif dat == 'conref':\n","    mapping = {'AGAINST': 1, 'FAVOR': 2, 'NONE': 0}\n","  return mapping"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2uFocs47egAC"},"outputs":[],"source":["def set_cols(dat):\n","  if dat == 'euro':\n","    mapping = {'parent_user_screen_name': 'parent'}\n","  elif dat == 'timme':\n","    mapping = {'root': 'parent'}\n","  elif dat == 'cd':\n","    mapping = {'text': 'rawTweet', 'author': 'name', 'parent_author': 'parent'}\n","  elif dat == 'conref':\n","    mapping = None\n","  return mapping"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"xKu3fxfusq9l"},"outputs":[],"source":["def update_dict_app(k, v, d):\n","  if k not in d.keys():\n","    d[k] = [v]\n","  else:\n","    temp = d[k]\n","    temp.append(v)\n","    d[k] = temp"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"M9YQCPv0buEo"},"outputs":[],"source":["def filter_kws(word):\n","  return len(word) >= 2"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"VG0CgIKNa9Dx"},"outputs":[],"source":["def tokenize(text, stopwords): # adapted from InfoVGAE\n","  original_text = str(text).lower()\n","  tok = original_text.split(' ')\n","  text = u''\n","  for x in tok:\n","    if len(x) == 0:\n","        continue\n","    elif x[0:4] == 'http' or x[0:5] == 'https':\n","        continue\n","    elif x[0] == '@':\n","        continue\n","    elif x in stopwords:\n","        continue\n","    text = text + ' ' + x\n","  translate_to = u' '\n","\n","  word_sep = u\" ,.?:;'\\\"/<>`!$%^&*()-=+~[]\\\\|{}()\\n\\t\" \\\n","              + u\"©℗®℠™،、⟨⟩‒–—―…„“”–――»«›‹‘’：（）！？=【】　・\" \\\n","              + u\"⁄·† ‡°″¡¿÷№ºª‰¶′″‴§|‖¦⁂❧☞‽⸮◊※⁀「」﹁﹂『』﹃﹄《》―—\" \\\n","              + u\"“”‘’、，一。►…¿«「」ー⋘▕▕▔▏┈⋙一ー।;!؟\"\n","  word_sep = u'#' + word_sep\n","  translate_table = dict((ord(char), translate_to) for char in word_sep)\n","  tokens = text.translate(translate_table).split(' ')\n","  filtered = [word for word in tokens if filter_kws(word)]\n","  return ' '.join(sorted(filtered))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"zJuHECJ9YMwt"},"outputs":[],"source":["def processTweets(df, stopwords):\n","  new_list = []\n","  tweets = list(df.rawTweet)\n","  for tweet in tweets:\n","    cleaned = tokenize(tweet, stopwords)\n","    new_list.append(cleaned)\n","  df['postTweet'] = new_list\n","  df['n_key'] = df.postTweet.apply(lambda x: len(x.split()))\n","  return df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def filterDat(df):\n","    df = df[df.n_key >= 5]\n","    userDict = dict()\n","    for u in df['name'].values:\n","        try:\n","            userDict[u] += 1\n","        except:\n","            userDict[u] = 1\n","\n","    pickedUsers = np.array(list(userDict.keys()))[np.where(np.array(list(userDict.values())) >= 3)]\n","    df = df[df['name'].isin(pickedUsers)]\n","    df.reset_index(drop=True, inplace=True)\n","    return df"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def combine_opp(edge_list):\n","    edges = []\n","    for edge in edge_list:\n","        opp = (edge[1], edge[0])\n","        if (opp not in edges) and (edge not in edges):\n","            edges.append((edge[0], edge[1]))\n","    return edges"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"oA-Ib-orsohz"},"source":["### Data Loaders/Builders"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"K0ugLEMLZKEe"},"outputs":[],"source":["def load_cd_as_df(cd_path):\n","  # get names of topics\n","  topics = [ f.name for f in os.scandir(cd_path + 'topics/') if f.is_dir() ]\n","  dfs = []\n","  cols_m = ['topic', 'CID', 'ID', 'PID', 'stance', 'rebuttal', 'author', 'parent_author']\n","  cols_d = ['topic', 'CID', 'ID', 'text']\n","  for topic in topics:\n","    # get names of each file group\n","    topic_metas = [ file for file in glob.glob(cd_path + 'topics/' + topic + '/*.meta') ]\n","    topic_datas = [ file for file in glob.glob(cd_path + 'topics/' + topic + '/*.data') ]\n","    topic_authors = [ file for file in glob.glob(cd_path + 'authors/' + topic + '/*.author') ]\n","    # build a nested dictionary for the conversations, ids to author names\n","    author_dict = {}\n","    for author_list in topic_authors:\n","      f = open(author_list, \"r\", encoding='utf8')\n","      cid = re.search(r\"(\\w)(?:.author$)\", author_list).group(1)\n","      author_dict[cid] = {}\n","      for line in f:\n","        id = re.search(r\"(?:\\w)(\\d+)\", line).group(1)\n","        author = re.search(r\"(?:^\\w\\d+\\s)(.+)\", line).group(1)\n","        author_dict[cid][id] = author\n","    # build a df with the meta information, adding in the author and parent author names\n","    dfm = []\n","    for meta in topic_metas:\n","      cid = re.search(r\"([A-Z])(?:\\d+.meta$)\", meta).group(1)\n","      f = open(meta, \"r\", encoding='utf8')\n","      id = f.readline()\n","      pid = f.readline()\n","      stance = f.readline()\n","      rebuttal = f.readline()\n","      id = re.search(r\"(?:ID=)(.+)\", id).group(1)\n","      pid = re.search(r\"(?:PID=)(.+)\", pid).group(1)\n","      if re.search(r\"(?:Stance=)(.+)\", stance):\n","        stance = re.search(r\"(?:Stance=)(.+)\", stance).group(1)\n","      else:\n","        stance = '0'\n","      rebuttal = re.search(r\"(?:rebuttal=)(.+)\", rebuttal).group(1)\n","      if id in author_dict[cid].keys():\n","        author = author_dict[cid][id]\n","      else:\n","        author = 'unknown'\n","      if pid != '-1':\n","        if pid in author_dict[cid].keys():\n","          parent_author = author_dict[cid][pid]\n","        else:\n","          parent_author = 'unknown'\n","      else:\n","        parent_author = 'null'\n","      df = pd.DataFrame([[str(topic), str(cid), int(id), int(pid), int(stance), str(rebuttal), str(author), str(parent_author)]], columns = cols_m)\n","      dfm.append(df)\n","    df1 = pd.concat(dfm)\n","    # build a df with the text information\n","    dfd = []\n","    for data in topic_datas:\n","      cid = re.search(r\"([A-Z])(?:\\d+.data$)\", data).group(1)\n","      id = int(re.search(r\"(?:\\w)(\\d+)(?:.data)\", data).group(1))\n","      f = open(data, \"r\", encoding='utf8')\n","      text = str(f.read())\n","      df = pd.DataFrame([[str(topic), str(cid), int(id), str(text)]], columns = cols_d)\n","      dfd.append(df)\n","    df2 = pd.concat(dfd)\n","    df2.set_index(['topic', 'CID', 'ID'], inplace=True)\n","    # join using the shared columns of topic, cid, and id\n","    df_n = df1.join(df2, on=['topic', 'CID', 'ID'])\n","    dfs.append(df_n)\n","  df = pd.concat(dfs)\n","  df.reset_index(drop=True,inplace=True)\n","  return df"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def filter_cd(df):\n","    replied_to = []\n","    for entry in df.iloc:\n","        topic = entry.topic\n","        cid = entry.CID\n","        pid = str(entry.PID)\n","        replied_to.append(topic+cid+pid)\n","    keep = []\n","    for i in range(len(df)):\n","        topic = df.loc[i, 'topic']\n","        cid = df.loc[i, 'CID']\n","        id = str(df.loc[i, 'ID'])\n","        info = topic + cid + id\n","        if info in replied_to:\n","            keep.append(i)\n","    df = df[df.index.isin(keep)]\n","    df.reset_index(inplace=True, drop=True)\n","    return df"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"njtJtHU3y01L"},"outputs":[],"source":["def split_conref(df):\n","  data = []\n","  for entry in df.iloc:\n","    user = entry.user\n","    stance = entry.stance\n","    if not np.isnan(entry.tweet_id):\n","      tweet_id = entry.tweet_id\n","      parent = None\n","      text = entry.tweet_text\n","      data.append([user, stance, tweet_id, parent, text, 'tweet'])\n","    if not np.isnan(entry.tweet_quote_id):\n","      tweet_id = entry.tweet_quote_id\n","      parent = entry.tweet_quote_user\n","      text = entry.tweet_quote_text\n","      data.append([user, stance, tweet_id, parent, text, 'quote'])\n","    if not np.isnan(entry.rt_id):\n","      tweet_id = entry.rt_id\n","      text = entry.rt_text\n","      parent = re.search(r\"RT @(\\w+)\", text).group(1)\n","      data.append([user, stance, tweet_id, parent, text, 'retweet'])\n","    if not np.isnan(entry.rt_quote_id):\n","      tweet_id = entry.rt_quote_id\n","      parent = entry.rt_quote_user\n","      text = entry.rt_quote_text\n","      data.append([user, stance, tweet_id, parent, text, 'quote'])\n","    if not np.isnan(entry.reply_id):\n","      tweet_id = entry.reply_id\n","      parent = None\n","      text = entry.reply_text\n","      data.append([user, stance, tweet_id, parent, text, 'reply'])\n","    if not np.isnan(entry.reply_to_id):\n","      tweet_id = entry.reply_to_id\n","      parent = entry.reply_to_user\n","      text = entry.reply_to_text\n","      data.append([user, stance, tweet_id, parent, text, 'reply'])\n","    if not np.isnan(entry.reply_to_quote_id):\n","      tweet_id = entry.reply_to_quote_id\n","      parent = entry.reply_to_quote_user\n","      text = entry.reply_to_quote_text\n","      data.append([user, stance, tweet_id, parent, text, 'reply'])\n","  df1 = pd.DataFrame(data, columns=['name', 'label', 'tweet_id', 'parent', 'rawTweet', 'action_type'])\n","  return df1"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def load_all_tweets(file_path, wanted):\n","  # get names of topics\n","  user_tweets = [ file for file in glob.glob(file_path + 'tweets/*.txt') ]\n","  dfs = []\n","  for user in user_tweets:\n","    uid = int(user[18:-4])\n","    if uid in wanted:\n","      f = open(user, \"r\", encoding='utf8')\n","      tweets = []\n","      for line in f:\n","          tweets.append(line)\n","      df1 = pd.DataFrame(tweets, columns=['rawTweet'])\n","      df1['name'] = [uid] * len(df1)\n","      dfs.append(df1)\n","  df = pd.concat(dfs)\n","  df.reset_index(drop=True,inplace=True)\n","  return df"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def add_timme_extra(df, file_path, orig_names):\n","    labs = pd.read_csv(file_path + 'new_dict_cleaned.csv', sep='\\t')\n","    users = list(labs['twitter_id'])\n","    df1 = load_all_tweets(file_path, users)\n","    rt = []\n","    for i in range(len(df1)):\n","        text = df1.iloc[i,0]\n","        if re.search(r'RT @\\w+:', text):\n","            rt.append(i)\n","    rt_only = df1[df1.index.isin(rt)]\n","    rt_only.reset_index(inplace=True, drop=True)\n","    dict_labs = {}\n","    mapping = {'D': 1, 'R': 2}\n","    for entry in labs.iloc:\n","        dict_labs[entry.twitter_id] = entry.party\n","    lab_list = []\n","    for entry in rt_only.iloc:\n","        l = dict_labs[entry['name']]\n","        lab_list.append(l)\n","    rt_only['label'] = lab_list\n","    rt_only['label'] = rt_only['label'].map(mapping)\n","    name_dict = {}\n","    names = rt_only['name'].unique()\n","    for i in range(len(names)):\n","        name_dict[names[i]] = 'user_' + str(i)\n","    rt_only['name'] = rt_only['name'].map(name_dict)\n","    roots = []\n","    for entry in rt_only.iloc:\n","        text = entry.rawTweet\n","        if re.search(r\"RT @(\\S+):\", str(text)):\n","            name = re.search(r\"RT @(\\S+):\", str(text)).group(1)\n","        roots.append((entry.name, name))\n","    rootsy = [None for i in range(len(rt_only))]\n","    for id, name in roots:\n","        rootsy[id] = name\n","    rt_only['root'] = rootsy\n","    rt = []\n","    for i in range(len(rt_only)):\n","        root = rt_only.iloc[i,3]\n","        if root in orig_names:\n","            rt.append(i)\n","    rt_only = rt_only[rt_only.index.isin(rt)]\n","    rt_only.reset_index(inplace=True, drop=True)\n","    return pd.concat([df, rt_only], ignore_index=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Graph Build"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"jeDskCMVrxtN"},"outputs":[],"source":["def internal_build_base(df, name_dict):\n","  edge_list = []\n","  y_dict = {}\n","  interactions = {}\n","  for entry in df.iloc:\n","    user = name_dict[entry['name']]\n","    if entry.parent in name_dict.keys():\n","      avec = name_dict[entry.parent]\n","      if user != avec:\n","        edge_list.append((user, avec))\n","        if (user, avec) in interactions.keys():\n","          interactions[(user, avec)] += 1\n","        elif (avec, user) in interactions.keys():\n","          interactions[(avec, user)] += 1\n","        else:\n","          interactions[(user, avec)] = 1\n","    y = entry.label\n","    update_dict_app(user, y, y_dict)\n","  return edge_list, y_dict, interactions"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"_eITZT_DtTiQ"},"outputs":[],"source":["def edges_build_base(edge_list, interactions):\n","  fin_edges = []\n","  edge_list = set(edge_list)\n","  edge_list = combine_opp(edge_list)\n","  for edge in edge_list:\n","    count = 0\n","    if (edge[0], edge[1]) in interactions.keys():\n","      count += interactions[(edge[0], edge[1])]\n","    elif (edge[1], edge[0]) in interactions.keys():\n","      count += interactions[(edge[1], edge[0])]\n","    fin_edges.append((edge[0], edge[1], {'weight' : count}))\n","  return fin_edges"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"TeEbwK2UvhEz"},"outputs":[],"source":["def build_mapping_df_base(df, name_dict, G):\n","  mentions = []\n","  hashtags = []\n","  for entry in df.iloc:\n","      text = entry.rawTweet\n","      m = re.findall(r'(@\\w+)', text)\n","      h = re.findall(r'#\\w+', text)\n","      mentions.append(m)\n","      hashtags.append(h)\n","  df['mentions'] = mentions\n","  df['hashtags'] = hashtags\n","  keep_cols = ['name', 'rawTweet', 'postTweet', 'label', 'mentions', 'hashtags']\n","  mapping = df[keep_cols]\n","  mapping['id'] = mapping['name'].map(name_dict)\n","  cols = ['id', 'name', 'rawTweet', 'postTweet', 'label', 'mentions', 'hashtags']\n","  mapping = mapping[cols]\n","  nodes = list(G.nodes())\n","  mapping = mapping[mapping.id.isin(nodes)]\n","  return mapping"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"lLx6hEqLGuo7"},"outputs":[],"source":["def build_graph_normal(df):\n","  # build child name to id dict\n","  names = list(df['name'].unique())\n","  name_dict = {}\n","  for i in range(len(names)):\n","    name_dict[names[i]] = i\n","\n","  edge_list, y_dict, interactions = internal_build_base(df, name_dict)\n","\n","  # build labels\n","  for key in sorted(y_dict.keys()):\n","    vals = y_dict[key]\n","    if len(vals) == 1:\n","      y_dict[key] = int(vals[0])\n","    elif len(set(vals)) == 1:\n","      y_dict[key] = int(vals[0])\n","    else:\n","      y_dict[key] = int(mode(vals))\n","\n","  fin_edges = edges_build_base(edge_list, interactions)\n","\n","  # build graph\n","  G = nx.Graph()\n","  G.add_edges_from(fin_edges)\n","\n","  df1 = build_mapping_df_base(df, name_dict, G)\n","  return G, df, df1, y_dict"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def build_bhin(df):\n","  # make user to index dict\n","  names = list(df['name'].unique())\n","  name_dict = {}\n","  for i in range(len(names)):\n","    name_dict[names[i]] = i\n","\n","  # make tweet to index dict (starting from last idx of users)\n","  tweets = list(df.postTweet.unique())\n","  tweet_dict = {}\n","  last_idx = len(names) - 1\n","  for i in range(len(tweets)):\n","    tweet_dict[tweets[i]] = last_idx + 1\n","    last_idx += 1\n","\n","  # build the edge list\n","  edge_list = []\n","  for entry in df.iloc:\n","    postTweet = entry[\"postTweet\"]\n","    tweet_idx = tweet_dict[postTweet]\n","    user_name = entry[\"name\"]\n","    user_idx = name_dict[user_name]\n","    edge_list.append((user_idx, tweet_idx))\n","\n","  # make the bipartite graph from it\n","  H = nx.Graph()\n","  H.add_nodes_from(name_dict.values(), bipartite = 0)\n","  H.add_nodes_from(tweet_dict.values(), bipartite = 1)\n","  H.add_edges_from(edge_list)\n","\n","  return H"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Main Code"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"JPTDsw9ZT7k0"},"outputs":[],"source":["def build_dataset(dat = 'euro', typ = 'pc', cd_top = 'all', filter = True, t_all=False,\n","                  output_path = './output', folder_path = './Datasets/'):\n","  \"\"\"\n","  PARAMETERS : \n","    dat : str; the dataset to be used; can be 'euro', 'timme', 'cd', or 'conref'; defaults to 'euro' if left empty or invalid string entered\n","    typ : str; the label set to be used; can be 'full' or 'pc'; defaults to 'pc' if left empty or invalid string entered\n","    cd_top : str; the topic to use if using 'cd' dataset; can be 'all', 'abortion', 'marijuana', 'gayRights', or 'obama'; defaults to 'all' if left empty or invalid string entered\n","    filter : boolean; whether or not to filter the users and tweets like InfoVGAE; defaults to True\n","    t_all : boolean; whether or not to build TIMME as P_all; defaults to False\n","    output_path : str; the location to output files to; defaults to './output'\n","    folder_path : str; the folder in which the files have been placed; defaults to './Datasets/'\n","  \"\"\"\n","  # verify inputs, set mapping for labels, set mapping for column names, and make sure output path exists\n","  dat, typ, cd_top, act = check_inputs(dat, typ, cd_top, act)\n","  mapping = set_mapping(dat)\n","  col_change = set_cols(dat)\n","  if output_path == './output':\n","    os.makedirs(output_path, exist_ok=True)\n","\n","  # load stopwords if format is graph\n","  if dat in ['euro', 'timme', 'cd']:\n","    stopword_path = './Stopwords/stopwords_en.txt'\n","  else:\n","    stopword_path = './Stopwords/stopwords_it.txt'\n","  stopwords = []\n","  with open(stopword_path, 'r') as infile:\n","    for word in infile.readlines():\n","        stopwords.append(word[:-1])\n","\n","  # load dataset as a dataframe\n","  if dat != 'cd':\n","    path = folder_path + 'data_' + dat + '.csv'\n","    if dat in ['euro', 'timme']:\n","      sep = '\\t'\n","    else:\n","      sep = ','\n","    df = pd.read_csv(path, sep=sep)\n","  else:\n","    path = folder_path + 'CreateDebate/'\n","    df = load_cd_as_df(path)\n","    if cd_top != 'all':\n","      df = df[df.topic == cd_top]\n","  \n","  # if conref data, split the lines properly\n","  if dat == 'conref':\n","    df = split_conref(df)\n","\n","  # if using timme, load the user dictionary and add to the dataframe, then build roots\n","  if dat == 'timme':\n","    dict_file = pd.read_csv(folder_path + 'dict_timme.csv', sep='\\t')\n","    dict_t = {}\n","    for entry in dict_file.iloc:\n","      dict_t[entry.twitter_id] = entry.twitter_name\n","    df['name'] = df['name'].map(dict_t)\n","    roots = []\n","    for entry in df.iloc:\n","      text = entry.rawTweet\n","      if re.search(r\"RT @(\\S+):\", str(text)):\n","        name = re.search(r\"RT @(\\S+):\", str(text)).group(1)\n","        roots.append((entry.name, name))\n","    rootsy = [None for i in range(len(df))]\n","    for id, name in roots:\n","      rootsy[id] = name\n","    df['root'] = rootsy\n","  \n","  # if using timme all, load the additional info and combine the dataframes\n","  if t_all and dat == 'timme':\n","    df = add_timme_extra(df, folder_path, list(df['name'].unique()))\n","  \n","  # remap labels\n","  if dat == 'cd':\n","    label_name = 'stance'\n","  else:\n","    label_name = 'label'\n","  if mapping is not None:\n","    df['label'] = df[label_name].map(mapping)\n","  \n","  # rename columns and add tweet_id column if doesn't exist\n","  if col_change is not None:\n","    df.rename(columns=col_change, inplace=True)\n","  if 'tweet_id' not in df.columns:\n","    df['tweet_id'] = df.index\n","\n","  # drop any entries without labels or names or text\n","  df = df.dropna(axis = 0, how ='any', subset=['label', 'name', 'rawTweet'])\n","\n","  # filter df if only looking at pro/con, otherwise just typecast the labels to ints\n","  if typ == 'pc':\n","    df = df.astype({'label':'int'})\n","    df = df[df.label != 0]\n","  else:\n","    df = df.astype({'label':'int'})\n","  \n","  df = processTweets(df, stopwords)\n","  df = df.dropna(axis = 0, how ='any', subset=['postTweet'])\n","  if filter:\n","    df = filterDat(df)\n","  \n","  # time to build the graph\n","  # build the graph based on interactions, plus the bhin for bipartite potential\n","  H = build_bhin(df)\n","  G, df, df1, y_dict = build_graph_normal(df)\n","\n","  # if using createdebate, add the topic to dat for naming of files\n","  if dat == 'cd':\n","    dat = dat + cd_top\n","  \n","  # if building t_all, adjust the filename\n","  if dat == 'timme':\n","    if t_all:\n","      dat = dat + '_all'\n","\n","  # write out files\n","  df.to_csv(output_path+'/'+dat+'_data.csv', sep='\\t')\n","  df1.to_csv(output_path+'/'+dat+'_mapping.csv', sep='\\t', index=False)\n","  nx.write_edgelist(H, output_path+'/'+dat+'_bipartite.txt')\n","  nx.write_weighted_edgelist(G, output_path+'/'+dat+'_graph.txt')\n","  with open(output_path+'/'+dat+'_ydict.json', \"w\") as fp:\n","      json.dump(y_dict,fp)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7HE5SO7lT73R"},"source":["## Running"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26319,"status":"ok","timestamp":1681897235778,"user":{"displayName":"Maia Sutter","userId":"04037068172289567548"},"user_tz":-120},"id":"YG2JslIsT9bU","outputId":"e02d4fc7-7739-4010-9f08-bf85538a304f"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\maiad\\AppData\\Local\\Temp\\ipykernel_8992\\132103042.py:14: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  mapping['id'] = mapping['name'].map(name_dict)\n"]}],"source":["# options : 'euro', 'timme', 'cd', 'conref'\n","dat = 'euro'\n","# options : 'all', 'abortion', 'marijuana', 'gayRights', or 'obama'\n","cd_top = 'all'\n","# options : True or False\n","t_all = False\n","# options : True or False\n","filter = True\n","file_path = './Processed/'\n","\n","build_dataset(dat=dat, cd_top=cd_top, filter=filter, t_all=t_all, output_path=file_path)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNwwZSgd5RBaQrVB7SbKXYk","collapsed_sections":["BP8O9sm2slZf"],"mount_file_id":"1t-KVzgjev0gyDukJCSUnkkmL0CTgwgks","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}
